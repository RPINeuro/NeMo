@article{Amir2013,
abstract = {Marching along the DARPA SyNAPSE roadmap, IBM unveils a trilogy of innovations towards the TrueNorth cognitive computing system inspired by the brain's function and efficiency. The sequential programming paradigm of the von Neumann architecture is wholly unsuited for TrueNorth. Therefore, as our main contribution, we develop a new programming paradigm that permits construction of complex cognitive algorithms and applications while being efficient for TrueNorth and effective for programmer productivity. The programming paradigm consists of (a) an abstraction for a TrueNorth program, named Corelet, for representing a network of neurosynaptic cores that encapsulates all details except external inputs and outputs; (b) an object-oriented Corelet Language for creating, composing, and decomposing corelets; (c) a Corelet Library that acts as an ever-growing repository of reusable corelets from which programmers compose new corelets; and (d) an end-to-end Corelet Laboratory that is a programming environment which integrates with the TrueNorth architectural simulator, Compass, to support all aspects of the programming cycle from design, through development, debugging, and up to deployment. The new paradigm seamlessly scales from a handful of synapses and neurons to networks of neurosynaptic cores of progressively increasing size and complexity. The utility of the new programming paradigm is underscored by the fact that we have designed and implemented more than 100 algorithms as corelets for TrueNorth in a very short time span.},
annote = {Has information on the prog. paradigm involved in True North,},
author = {Amir, Arnon and Datta, Pallab and Risk, William P. and Cassidy, Andrew S. and Kusnitz, Jeffrey a. and Esser, Steve K. and Andreopoulos, Alexander and Wong, Theodore M. and Flickner, Myron and Alvarez-Icaza, Rodrigo and McQuinn, Emmett and Shaw, Ben and Pass, Norm and Modha, Dharmendra S.},
doi = {10.1109/IJCNN.2013.6707078},
file = {:Users/plaggm/Google Drive/ResearchPapers/Amir et al/Proceedings of the International Joint Conference on Neural Networks/Amir et al.{\_}2013{\_}Cognitive computing programming paradigm A Corelet Language for composing networks of neurosynaptic cores.pdf:pdf},
isbn = {9781467361293},
journal = {Proceedings of the International Joint Conference on Neural Networks},
number = {December},
title = {{Cognitive computing programming paradigm: A Corelet Language for composing networks of neurosynaptic cores}},
year = {2013}
}
@article{Brette2007,
author = {Brette, Romain and Rudolph, Michelle and Carnevale, Ted and Hines, Michael and Beeman, David and Bower, James M. and Diesmann, Markus and Morrison, Abigail and Goodman, Philip H. and Harris, Frederick C. and Zirpe, Milind and Natschl{\"{a}}ger, Thomas and Pecevski, Dejan and Ermentrout, Bard and Djurfeldt, Mikael and Lansner, Anders and Rochel, Olivier and Vieville, Thierry and Muller, Eilif and Davison, Andrew P. and {El Boustani}, Sami and Destexhe, Alain},
doi = {10.1007/s10827-007-0038-6},
file = {:Users/plaggm/Google Drive/ResearchPapers/Brette et al/Journal of Computational Neuroscience/Brette et al.{\_}2007{\_}Simulation of networks of spiking neurons A review of tools and strategies.pdf:pdf},
issn = {1573-6873},
journal = {Journal of Computational Neuroscience},
number = {3},
pages = {349--398},
title = {{Simulation of networks of spiking neurons: A review of tools and strategies}},
url = {http://link.springer.com/10.1007/s10827-007-0038-6},
volume = {23},
year = {2007}
}
@article{Brown2014,
author = {Brown, Andrew D. and Mills, Rob M. and Dugan, Kier J. and Reeve, Jeff S. and Furber, Steve B.},
doi = {10.1049/iet-cdt.2014.0110},
file = {:Users/plaggm/Google Drive/ResearchPapers/Brown et al/Designing with Uncertainty Workshop/Brown et al.{\_}2014{\_}Reliable computation with unreliable computers.pdf:pdf},
issn = {1751-8601},
journal = {Designing with Uncertainty Workshop},
pages = {230--237},
title = {{Reliable computation with unreliable computers}},
year = {2014}
}
@inproceedings{Cassidy:2014:RSC:2683593.2683597,
address = {Piscataway, NJ, USA},
author = {Cassidy, Andrew S and Alvarez-Icaza, Rodrigo and Akopyan, Filipp and Sawada, Jun and Arthur, John V and Merolla, Paul A and Datta, Pallab and Tallada, Marc Gonzalez and Taba, Brian and Andreopoulos, Alexander and Amir, Arnon and Esser, Steven K and Kusnitz, Jeff and Appuswamy, Rathinakumar and Haymes, Chuck and Brezzo, Bernard and Moussalli, Roger and Bellofatto, Ralph and Baks, Christian and Mastro, Michael and Schleupen, Kai and Cox, Charles E and Inoue, Ken and Millman, Steve and Imam, Nabil and McQuinn, Emmett and Nakamura, Yutaka Y and Vo, Ivan and Guo, Chen and Nguyen, Don and Lekuch, Scott and Asaad, Sameh and Friedman, Daniel and Jackson, Bryan L and Flickner, Myron D and Risk, William P and Manohar, Rajit and Modha, Dharmendra S},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
doi = {10.1109/SC.2014.8},
file = {:Users/plaggm/Google Drive/ResearchPapers/Cassidy et al/Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis/Cassidy et al.{\_}2014{\_}Real-time Scalable Cortical Computing at 46 Giga-synaptic OPSWatt with {\~{}}100{\&}Times Speedup in Time-to-solution and {\~{}}1.pdf:pdf},
isbn = {978-1-4799-5500-8},
pages = {27--38},
publisher = {IEEE Press},
series = {SC '14},
title = {{Real-time Scalable Cortical Computing at 46 Giga-synaptic OPS/Watt with {\~{}}100{\&}Times; Speedup in Time-to-solution and {\~{}}100,000{\&}Times; Reduction in Energy-to-solution}},
url = {http://dx.doi.org/10.1109/SC.2014.8},
year = {2014}
}
@article{Cassidy2013.101045,
abstract = {Marching along the DARPA SyNAPSE roadmap, IBM unveils a trilogy of innovations towards the TrueNorth cognitive computing system inspired by the brain's function and efficiency. Judiciously balancing the dual objectives of functional capability and implementation/operational cost, we develop a simple, digital, reconfigurable, versatile spiking neuron model that supports one-to-one equivalence between hardware and simulation and is implementable using only 1272 ASIC gates. Starting with the classic leaky integrate-and-fire neuron, we add: (a) configurable and reproducible stochasticity to the input, the state, and the output; (b) four leak modes that bias the internal state dynamics; (c) deterministic and stochastic thresholds; and (d) six reset modes for rich finite-state behavior. The model supports a wide variety of computational functions and neural codes. We capture 50+ neuron behaviors in a library for hierarchical composition of complex computations and behaviors. Although designed with cognitive algorithms and applications in mind, serendipitously, the neuron model can qualitatively replicate the 20 biologically-relevant behaviors of a dynamical neuron model.},
author = {Cassidy, Andrew S. and Merolla, Paul and Arthur, John V. and Esser, Steve K. and Jackson, Bryan and Alvarez-Icaza, Rodrigo and Datta, Pallab and Sawada, Jun and Wong, Theodore M. and Feldman, Vitaly and Amir, Arnon and Rubin, Daniel Ben Dayan and Akopyan, Filipp and McQuinn, Emmett and Risk, William P. and Modha, Dharmendra S.},
doi = {10.1109/IJCNN.2013.6707077},
file = {:Users/plaggm/Google Drive/ResearchPapers/Cassidy et al/Proceedings of the International Joint Conference on Neural Networks/Cassidy et al.{\_}2013{\_}Cognitive computing building block A versatile and efficient digital neuron model for neurosynaptic cores.pdf:pdf},
isbn = {9781467361293},
journal = {Proceedings of the International Joint Conference on Neural Networks},
title = {{Cognitive computing building block: A versatile and efficient digital neuron model for neurosynaptic cores}},
year = {2013}
}
@article{Chen2014,
author = {Chen, Zhengzhang and Son, Seung Woo and Hendrix, William and Agrawal, Ankit and Liao, Wei-keng and Choudhary, Alok},
doi = {10.1109/SC.2014.65},
file = {:Users/plaggm/Google Drive/ResearchPapers/Chen et al/SuperComputing/Chen et al.{\_}2014{\_}NUMARCK Machine Learning Algorithm for Resiliency and Checkpointing.pdf:pdf},
isbn = {978-1-4799-5500-8},
journal = {SuperComputing},
title = {{NUMARCK : Machine Learning Algorithm for Resiliency and Checkpointing}},
year = {2014}
}
@article{Delorme2003,
abstract = {Many biological neural network models face the problem of scalability because of the limited computational power of today's computers. Thus, it is difficult to assess the efficiency of these models to solve complex problems such as image processing. Here, we describe how this problem can be tackled using event-driven computation. Only the neurons that emit a discharge are processed and, as long as the average spike discharge rate is low, millions of neurons and billions of connections can be modelled. We describe the underlying computation and implementation of such a mechanism in SpikeNET, our neural network simulation package. The type of model one can build is not only biologically compliant, it is also computationally efficient as 400 000 synaptic weights can be propagated per second on a standard desktop computer. In addition, for large networks, we can set very small time steps (< 0.01 ms) without significantly increasing the computation time. As an example, this method is applied to solve complex cognitive tasks such as face recognition in natural images.},
author = {Delorme, Arnaud and Thorpe, Simon J},
doi = {10.1088/0954-898X{\_}14{\_}4{\_}301},
file = {:Users/plaggm/Google Drive/ResearchPapers/Delorme, Thorpe/Network (Bristol, England)/Delorme, Thorpe{\_}2003{\_}SpikeNET an event-driven simulation package for modelling large networks of spiking neurons.pdf:pdf},
isbn = {0954-898X (Print)$\backslash$r0954-898X (Linking)},
issn = {0954-898X},
journal = {Network (Bristol, England)},
keywords = {Action Potentials,Action Potentials: physiology,Algorithms,Computer Simulation,Computer Systems,Face,Humans,Models,Neural Networks (Computer),Neural Pathways,Neurological,Neurons,Neurons: physiology,Random Allocation,Retina,Retina: physiology,Software,Stochastic Processes,Synapses},
number = {4},
pages = {613--27},
pmid = {14653495},
title = {{SpikeNET: an event-driven simulation package for modelling large networks of spiking neurons.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14653495},
volume = {14},
year = {2003}
}
@article{goodman2008brian,
author = {Goodman, Dan and Brette, Romain},
file = {:Users/plaggm/Google Drive/ResearchPapers/Goodman, Brette/Frontiers in neuroinformatics/Goodman, Brette{\_}2008{\_}Brian a simulator for spiking neural networks in Python.pdf:pdf},
journal = {Frontiers in neuroinformatics},
publisher = {Frontiers Research Foundation},
title = {{Brian: a simulator for spiking neural networks in Python}},
volume = {2},
year = {2008}
}
@inproceedings{HanTaha2014,
abstract = {There is currently a strong push in the research community to develop biological scale implementations of neuron based vision models. Systems at this scale are computationally demanding and have generally utilized more accurate neuron models, such as the Izhikevich and Hodgkin- Huxley models, in favor of the integrate and fire model. This paper examines the feasibility of using a cluster of NVIDIA General Purpose Graphics Processing Units (GPGPUs) for accelerating a spiking neural network based character recognition network based on the Izhikevich and Hodgkin-Huxley models to enable such large scale systems. We utilized a 32 node cluster at NCSA containing an NVIDIA Tesla S1070 GPGPU on each node. Based on a thorough review of the literature, this is the first study examining the use of a cluster of GPGPUs for accelerating neuromorphic models. Our results show that the GPGPU can provide speedups of 24.6 and 177.0 times over a dual core 2.4 GHz AMD Opteron processor for the Izhikevich and Hodgkin-Huxley models respectively. Additionally, the MPI implementations of the models scaled almost linearly, with 16 GPGPUs providing throughputs of 14.1 and 15.9 times that of a single GPGPU for the Izhikevich and Hodgkin-Huxley models respectively. This indicates that clusters of GPGPUs are well suited for this application domain.},
author = {Han, Bing and Taha, Tarek M.},
booktitle = {The 2010 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2010.5596803},
file = {:Users/plaggm/Google Drive/ResearchPapers/Han, Taha/The 2010 International Joint Conference on Neural Networks (IJCNN)/Han, Taha{\_}2010{\_}Neuromorphic models on a GPGPU cluster.pdf:pdf},
isbn = {978-1-4244-6916-1},
keywords = {Acceleration,Biological system modeling,Book reviews,Computational modeling,GPGPU cluster,Hodgkin- Huxley model,Izhikevich model,NVIDIA,Neurons,biocomputing,character recognition,computer graphic equipment,coprocessors,graphics processing unit,neural nets,neuromorphic model,neuron based vision model,neurophysiology,pattern clustering,spiking neural network},
month = {jul},
number = {1},
pages = {1--8},
publisher = {IEEE},
title = {{Neuromorphic models on a GPGPU cluster}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5596803},
year = {2010}
}
@article{Hereld2004,
abstract = {Simulations of large neural networks have the potential to contribute uniquely to the study of epilepsy, from the effects of extremely local changes in neuron environment and behavior, to the effects of large scale wiring anomalies. Currently, simulations with sufficient detail in the neuron model, however, are limited to cell counts that are far smaller than scales measured by typical probes. Furthermore, it is likely that future simulations will follow the path that large-scale simulations in other fields have and include hierarchically interacting components covering different scales and different biophysics. The resources needed for problem solving in this domain call for petascale computing--computing with supercomputers capable of 10(15) operations a second and holding datasets of 10(15) bytes in memory. We will lay out the structure of our simulation of epileptiform electrical activity in the neocortex, describe experiments and models of its scaling behavior in large cluster supercomputers, identify tight spots in this behavior, and project the performance onto a candidate next generation computing platform.},
author = {Hereld, M and Stevens, R L and van Drongelen, W and Lee, H C},
doi = {10.1109/IEMBS.2004.1404117},
file = {:Users/plaggm/Google Drive/ResearchPapers/Hereld et al/Conference proceedings ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engine. Conference/Hereld et al.{\_}2004{\_}Developing a petascale neural simulation.pdf:pdf},
isbn = {0-7803-8439-3},
issn = {1557-170X},
journal = {Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Conference},
keywords = {epilepsy,high,neocortex,neural modeling,performance computing,simulation},
pages = {3999--4002},
pmid = {17271175},
title = {{Developing a petascale neural simulation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17271175},
volume = {6},
year = {2004}
}
@article{Ho2008,
abstract = {Recently, cellular neural networks (CNNs) have been demonstrated to be a highly effective paradigm applicable in a wide range of areas. Typically, CNNs can be implemented using VLSI circuits, but this would unavoidably require additional hardware. On the other hand, we can also implement CNNs purely by software; this, however, would result in very low performance when given a large CNN problem size. Nowadays, conventional desktop computers are usually equipped with programmable graphics processing units (GPUs) that can support parallel data processing. This paper introduces a GPU-based CNN simulator. In detail, we carefully organize the CNN data as 4-channel textures, and efficiently implement the CNN computation as fragment programs running in parallel on a GPU. In this way, we can create a high performance but low-cost CNN simulator. Experimentally, we demonstrate that the resultant GPU-based CNN simulator can run 8--17 times faster than a CPU-based CNN simulator.},
author = {Ho, Tze-Yui and Lam, Ping-Man and Leung, Chi-Sing},
doi = {10.1016/j.patcog.2008.01.018},
file = {:Users/plaggm/Google Drive/ResearchPapers/Ho, Lam, Leung/Pattern Recognition/Ho, Lam, Leung{\_}2008{\_}Parallelization of cellular neural networks on GPU.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {cellular neural networks,gpu,simd},
number = {8},
pages = {2684--2692},
title = {{Parallelization of cellular neural networks on GPU}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320308000605$\backslash$nhttp://www.sciencedirect.com/science/article/pii/S0031320308000605},
volume = {41},
year = {2008}
}
@inproceedings{Liu:2015:FML:2769458.2769474,
address = {London, United Kingdom},
author = {Liu, Ning and Haider, Adnan and Sun, Xian-he and Jin, Dong},
booktitle = {Proceedings of the 3rd ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
doi = {10.1145/2769458.2769474},
file = {:Users/plaggm/Google Drive/ResearchPapers/Liu et al/Proceedings of the 3rd ACM SIGSIM Conference on Principles of Advanced Discrete Simulation/Liu et al.{\_}2015{\_}FatTreeSim Modeling Large-scale Fat-Tree Networks for HPC Systems and Data Centers Using Parallel and Discrete Event Sim.pdf:pdf},
isbn = {9781450335836},
keywords = {{blue gene/q,datacenter interconnection network,fat-tree networks,parallel discrete event simulation,supercompute interconnection networks{\}}},
pages = {199--210},
publisher = {ACM Press},
title = {{FatTreeSim: Modeling Large-scale Fat-Tree Networks for HPC Systems and Data Centers Using Parallel and Discrete Event Simulation}},
url = {http://dl.acm.org/citation.cfm?doid=2769458.2769474},
year = {2015}
}
@article{Lobb2005,
abstract = { Neural systems are composed of a large number of highly-connected neurons and are widely simulated within the neurological community. In this paper, we examine the application of parallel discrete event simulation techniques to networks of a complex model called the Hodgkin-Huxley neuron. We describe the conversion of this model into an event-driven simulation, a technique that offers the potential of much greater performance in parallel and distributed simulations compared to time-stepped techniques. We report results of an initial set of experiments conducted to determine the feasibility of this parallel event-driven Hodgkin-Huxley model and analyze its viability for large-scale neural simulations.},
author = {Lobb, Collin J. and Chao, Zenas and Fujimoto, Richard M. and Potter, Steve M.},
doi = {10.1109/PADS.2005.18},
isbn = {0-7695-2383-8},
issn = {1087-4097},
journal = {Proceedings - Workshop on Principles of Advanced and Distributed Simulation, PADS},
pages = {16--25},
title = {{Parallel event-driven neural network simulations using the Hodgkin-Huxley neuron model}},
year = {2005}
}
@article{Makino2003,
abstract = {Efficient simulation techniques for a discrete-event pulsed neural network simulator are developed. In a discrete-event simulation framework, simulation of complex neural behaviours, such as phase precession and phase arbitration, demands the prediction of delayed firing times. The new technique, the incremental partitioning method, uses linear envelopes of the state variable of a neuron to partition the simulated time so that the delayed-firing time is reliably calculated by applying the bisection-combined Newton-Raphson method to every partition. The quick filtering technique is also proposed for reducing calculation cost of linear envelopes. The simulator developed, Punnets, has achieved efficiency and precision, but is still capable of simulating a complex behaviour of large-scale neural network models.},
author = {Makino, T.},
doi = {10.1007/s00521-003-0358-z},
file = {:Users/plaggm/Google Drive/ResearchPapers/Makino/Neural Computing and Applications/Makino{\_}2003{\_}A discrete-event neural network simulator for general neuron models.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Discrete-event simulation,Event-driven simulation,Incremental partitioning method,Neural network simulator,Pulsed neural network,Punnets},
pages = {210--223},
title = {{A discrete-event neural network simulator for general neuron models}},
volume = {11},
year = {2003}
}
@article{Mattia2000,
abstract = {A simulation procedure is described for making feasible large-scale simulations of recurrent neural networks of spiking neurons and plastic synapses. The procedure is applicable if the dynamic variables of both neurons and synapses evolve deterministically between any two successive spikes. Spikes introduce jumps in these variables, and since spike trains are typically noisy, spikes introduce stochasticity into both dynamics. Since all events in the simulation are guided by the arrival of spikes, at neurons or synapses, we name this procedure event-driven. The procedure is described in detail, and its logic and performance are compared with conventional (synchronous) simulations. The main impact of the new approach is a drastic reduction of the computational load incurred upon introduction of dynamic synaptic efficacies, which vary organically as a function of the activities of the pre- and postsynaptic neurons. In fact, the computational load per neuron in the presence of the synaptic dynamics grows linearly with the number of neurons and is only about 6{\%} more than the load with fixed synapses. Even the latter is handled quite efficiently by the algorithm. We illustrate the operation of the algorithm in a specific case with integrate-and-fire neurons and specific spike-driven synaptic dynamics. Both dynamical elements have been found to be naturally implementable in VLSI. This network is simulated to show the effects on the synaptic structure of the presentation of stimuli, as well as the stability of the generated matrix to the neural activity it induces.},
author = {Mattia, M and {Del Giudice}, P},
doi = {10.1162/089976600300014953},
file = {:Users/plaggm/Google Drive/ResearchPapers/Mattia, Del Giudice/Neural computation/Mattia, Del Giudice{\_}2000{\_}Efficient event-driven simulation of large networks of spiking neurons and dynamical synapses.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
number = {10},
pages = {2305--2329},
pmid = {11032036},
title = {{Efficient event-driven simulation of large networks of spiking neurons and dynamical synapses.}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976600300014953},
volume = {12},
year = {2000}
}
@article{merolla2014million,
author = {Merolla, Paul A and Arthur, John V and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Others},
file = {:Users/plaggm/Google Drive/ResearchPapers/Merolla et al/Science/Merolla et al.{\_}2014{\_}A million spiking-neuron integrated circuit with a scalable communication network and interface.pdf:pdf},
journal = {Science},
number = {6197},
pages = {668--673},
publisher = {American Association for the Advancement of Science},
title = {{A million spiking-neuron integrated circuit with a scalable communication network and interface}},
volume = {345},
year = {2014}
}
@article{Merolla2011,
abstract = {The grand challenge of neuromorphic computation is to develop a flexible brain-like architecture capable of a wide array of real-time applications, while striving towards the ultra-low power consumption and compact size of the human brain{\&}{\#}x2014;within the constraints of existing silicon and post-silicon technologies. To this end, we fabricated a key building block of a modular neuromorphic architecture, a neurosynaptic core, with 256 digital integrate-and-fire neurons and a 1024{\&}{\#}x00D7;256 bit SRAM crossbar memory for synapses using IBM's 45nm SOI process. Our fully digital implementation is able to leverage favorable CMOS scaling trends, while ensuring one-to-one correspondence between hardware and software. In contrast to a conventional von Neumann architecture, our core tightly integrates computation (neurons) alongside memory (synapses), which allows us to implement efficient fan-out (communication) in a naturally parallel and event-driven manner, leading to ultra-low active power consumption of 45pJ/spike. The core is fully configurable in terms of neuron parameters, axon types, and synapse states and is thus amenable to a wide range of applications. As an example, we trained a restricted Boltzmann machine offline to perform a visual digit recognition task, and mapped the learned weights to our chip.},
author = {Merolla, Paul and Arthur, John and Akopyan, Filipp and Imam, Nabil and Manohar, Rajit and Modha, Dharmendra S.},
doi = {10.1109/CICC.2011.6055294},
file = {:Users/plaggm/Google Drive/ResearchPapers/Merolla et al/Proceedings of the Custom Integrated Circuits Conference/Merolla et al.{\_}2011{\_}A digital neurosynaptic core using embedded crossbar memory with 45pJ per spike in 45nm.pdf:pdf},
isbn = {9781457702228},
issn = {08865930},
journal = {Proceedings of the Custom Integrated Circuits Conference},
pages = {1--4},
title = {{A digital neurosynaptic core using embedded crossbar memory with 45pJ per spike in 45nm}},
year = {2011}
}
@article{Migliore2006,
abstract = {The NEURON simulation environment has been extended to support parallel network simulations. Each processor integrates the equations for its subnet over an interval equal to the minimum (interprocessor) presynaptic spike generation to postsynaptic spike delivery connection delay. The performance of three published network models with very different spike patterns exhibits superlinear speedup on Beowulf clusters and demonstrates that spike communication overhead is often less than the benefit of an increased fraction of the entire problem fitting into high speed cache. On the EPFL IBM Blue Gene, almost linear speedup was obtained up to 100 processors. Increasing one model from 500 to 40,000 realistic cells exhibited almost linear speedup on 2,000 processors, with an integration time of 9.8 seconds and communication time of 1.3 seconds. The potential for speed-ups of several orders of magnitude makes practical the running of large network simulations that could otherwise not be explored.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Migliore, Michele and Cannia, C. and Lytton, W. W. and Markram, Henry and Hines, M. L.},
doi = {10.1007/s10827-006-7949-5},
eprint = {NIHMS150003},
file = {:Users/plaggm/Google Drive/ResearchPapers/Migliore et al/Journal of Computational Neuroscience/Migliore et al.{\_}2006{\_}Parallel network simulations with NEURON.pdf:pdf},
isbn = {1082700679495},
issn = {09295313},
journal = {Journal of Computational Neuroscience},
keywords = {Computer simulation,Parallel computation,Realistic modeling,Spiking networks},
number = {2},
pages = {119--129},
pmid = {16732488},
title = {{Parallel network simulations with NEURON}},
volume = {21},
year = {2006}
}
@article{Morrison2005,
abstract = {The availability of efficient and reliable simulation tools is one of the mission-critical technologies in the fast-moving field of computational neuroscience. Research indicates that higher brain functions emerge from large and complex cortical networks and their interactions. The large number of elements (neurons) combined with the high connectivity (synapses) of the biological network and the specific type of interactions impose severe constraints on the explorable system size that previously have been hard to overcome. Here we present a collection of new techniques combined to a coherent simulation tool removing the fundamental obstacle in the computational study of biological neural networks: the enormous number of synaptic contacts per neuron. Distributing an individual simulation over multiple computers enables the investigation of networks orders of magnitude larger than previously possible. The software scales excellently on a wide range of tested hardware, so it can be used in an interactive and iterative fashion for the development of ideas, and results can be produced quickly even for very large networks. In contrast to earlier approaches, a wide class of neuron models and synaptic dynamics can be represented.},
author = {Morrison, Abigail and Mehring, Carsten and Geisel, Theo and Aertsen, a D and Diesmann, Markus},
doi = {10.1162/0899766054026648},
file = {:Users/plaggm/Google Drive/ResearchPapers/Morrison et al/Neural computation/Morrison et al.{\_}2005{\_}Advancing the boundaries of high-connectivity network simulation with distributed computing.pdf:pdf},
isbn = {10.1162/0899766054026648},
issn = {0899-7667},
journal = {Neural computation},
number = {8},
pages = {1776--1801},
pmid = {15969917},
title = {{Advancing the boundaries of high-connectivity network simulation with distributed computing.}},
volume = {17},
year = {2005}
}
@article{Mount2010,
author = {Mount, David M},
file = {:Users/plaggm/Google Drive/ResearchPapers/Mount/Unknown/Mount{\_}2010{\_}ANN Programming Manual.pdf:pdf},
title = {{ANN Programming Manual}},
year = {2010}
}
@article{Nageswaran2009,
abstract = {Neural network simulators that take into account the spiking behavior of neurons are useful for studying brain mechanisms and for various neural engineering applications. Spiking Neural Network (SNN) simulators have been traditionally simulated on large-scale clusters, super-computers, or on dedicated hardware architectures. Alternatively, Compute Unified Device Architecture (CUDA) Graphics Processing Units (GPUs) can provide a low-cost, programmable, and high-performance computing platform for simulation of SNNs. In this paper we demonstrate an efficient, biologically realistic, large-scale SNN simulator that runs on a single GPU. The SNN model includes Izhikevich spiking neurons, detailed models of synaptic plasticity and variable axonal delay. We allow user-defined configuration of the GPU-SNN model by means of a high-level programming interface written in C++ but similar to the PyNN programming interface specification. PyNN is a common programming interface developed by the neuronal simulation community to allow a single script to run on various simulators. The GPU implementation (on NVIDIA GTX-280 with 1 GB of memory) is up to 26 times faster than a CPU version for the simulation of 100K neurons with 50 Million synaptic connections, firing at an average rate of 7 Hz. For simulation of 10 Million synaptic connections and 100K neurons, the GPU SNN model is only 1.5 times slower than real-time. Further, we present a collection of new techniques related to parallelism extraction, mapping of irregular communication, and network representation for effective simulation of SNNs on GPUs. The fidelity of the simulation results was validated on CPU simulations using firing rate, synaptic weight distribution, and inter-spike interval analysis. Our simulator is publicly available to the modeling community so that researchers will have easy access to large-scale SNN simulations.},
author = {Nageswaran, Jayram Moorkanikara and Dutt, Nikil and Krichmar, Jeffrey L. and Nicolau, Alex and Veidenbaum, Alexander V.},
doi = {10.1016/j.neunet.2009.06.028},
file = {:Users/plaggm/Google Drive/ResearchPapers/Nageswaran et al/Neural Networks/Nageswaran et al.{\_}2009{\_}A configurable simulation environment for the efficient simulation of large-scale spiking neural networks on grap.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {CUDA,Data parallelism,Graphics processor,Izhikevich spiking neuron,STDP},
number = {5-6},
pages = {791--800},
pmid = {19615853},
publisher = {Elsevier Ltd},
title = {{A configurable simulation environment for the efficient simulation of large-scale spiking neural networks on graphics processors}},
url = {http://dx.doi.org/10.1016/j.neunet.2009.06.028},
volume = {22},
year = {2009}
}
@article{Painkras2013,
author = {Painkras, Eustace and Plana, Luis A and Member, Senior and Garside, Jim and Temple, Steve and Galluppi, Francesco and Member, Student and Patterson, Cameron and Lester, David R and Brown, Andrew D and Member, Senior and Furber, Steve B},
file = {:Users/plaggm/Google Drive/ResearchPapers/Painkras et al/Unknown/Painkras et al.{\_}2013{\_}SpiNNaker A 1-W 18-Core System-on-Chip for Massively-Parallel Neural Network Simulation.pdf:pdf},
number = {8},
pages = {1943--1953},
title = {{SpiNNaker : A 1-W 18-Core System-on-Chip for Massively-Parallel Neural Network Simulation}},
volume = {48},
year = {2013}
}
@article{Pecevski2014,
abstract = {NEVESIM is a software package for event-driven simulation of networks of spiking neurons with a fast simulation core in C++, and a scripting user interface in the Python programming language. It supports simulation of heterogeneous networks with different types of neurons and synapses, and can be easily extended by the user with new neuron and synapse types. To enable heterogeneous networks and extensibility, NEVESIM is designed to decouple the simulation logic of communicating events (spikes) between the neurons at a network level from the implementation of the internal dynamics of individual neurons. In this paper we will present the simulation framework of NEVESIM, its concepts and features, as well as some aspects of the object-oriented design approaches and simulation strategies that were utilized to efficiently implement the concepts and functionalities of the framework. We will also give an overview of the Python user interface, its basic commands and constructs, and also discuss the benefits of integrating NEVESIM with Python. One of the valuable capabilities of the simulator is to simulate exactly and efficiently networks of stochastic spiking neurons from the recently developed theoretical framework of neural sampling. This functionality was implemented as an extension on top of the basic NEVESIM framework. Altogether, the intended purpose of the NEVESIM framework is to provide a basis for further extensions that support simulation of various neural network models incorporating different neuron and synapse types that can potentially also use different simulation strategies.},
author = {Pecevski, Dejan and Kappel, David and Jonke, Zeno},
doi = {10.3389/fninf.2014.00070},
file = {:Users/plaggm/Google Drive/ResearchPapers/Pecevski, Kappel, Jonke/Frontiers in neuroinformatics/Pecevski, Kappel, Jonke{\_}2014{\_}NEVESIM event-driven neural simulation framework with a Python interface.pdf:pdf},
issn = {1662-5196},
journal = {Frontiers in neuroinformatics},
keywords = {NEVESIM, neural simulator, spiking neurons, event-,event-driven,neural simulator,nevesim,python,spiking neurons},
number = {August},
pages = {70},
pmid = {25177291},
title = {{NEVESIM: event-driven neural simulation framework with a Python interface.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4132371{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {8},
year = {2014}
}
@article{Preissl2012,
abstract = {Inspired by the function, power, and volume of the organic brain, we are developing TrueNorth, a novel modu- lar, non-von Neumann, ultra-low power, compact architecture. TrueNorth consists of a scalable network of neurosynaptic cores, with each core containing neurons, dendrites, synapses, and axons. To set sail for TrueNorth, we developed Compass, a multi-threaded, massively parallel functional simulator and a parallel compiler that maps a network of long-distance pathways in the macaque monkey brain to TrueNorth. We demonstrate near-perfect weak scaling on a 16 rack IBM® Blue Gene®/Q (262144 CPUs, 256 TB memory), achieving an unprecedented scale of 256 million neurosynaptic cores containing 65 billion neurons and 16 trillion synapses running only 388× slower than real time with an average spiking rate of 8.1 Hz. By using emerging PGAS communication primitives, we also demonstrate 2× better real-time performance over MPI primitives on a 4 rack Blue Gene/P (16384 CPUs, 16 TB memory).},
annote = {Sims information},
author = {Preissl, Robert and Wong, Theodore M. and Datta, Pallab and Flickner, Myron and Singh, Raghavendra and Esser, Steven K. and Risk, William P. and Simon, Horst D. and Modha, Dharmendra S.},
doi = {10.1109/SC.2012.34},
file = {:Users/plaggm/Google Drive/ResearchPapers/Preissl et al/International Conference for High Performance Computing, Networking, Storage and Analysis, SC/Preissl et al.{\_}2012{\_}Compass A scalable simulator for an architecture for cognitive computing.pdf:pdf},
isbn = {9781467308069},
issn = {21674329},
journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
title = {{Compass: A scalable simulator for an architecture for cognitive computing}},
year = {2012}
}
@article{Rast2010a,
abstract = {Neural networks present a fundamentally different model of computation from the conventional sequential digital model. Modelling large networks on conventional hardware thus tends to be inefficient if not impossible. Neither dedicated neural chips, with model limitations, nor FPGA implementations, with scalability limitations, offer a satisfactory solution even though they have improved simulation performance dramatically. SpiNNaker introduces a different approach, the "neuromimetic" architecture, that maintains the neural optimisation of dedicated chips while offering FPGA-like universal configurability. Central to this parallel multiprocessor is an asynchronous event-driven model that uses interrupt-generating dedicated hardware on the chip to support real-time neural simulation. While this architecture is particularly suitable for spiking models, it can also implement "classical" neural models like the MLP efficiently. Nonetheless, event handling, particularly servicing incoming packets, requires careful and innovative design in order to avoid local processor congestion and possible deadlock. Using two exemplar models, a spiking network using Izhikevich neurons, and an MLP network, we illustrate how to implement efficient service routines to handle input events. These routines form the beginnings of a library of "drop-in" neural components. Ultimately, the goal is the creation of a library-based development system that allows the modeller to describe a model in a high-level neural description environment of his choice and use an automated tool chain to create the appropriate SpiNNaker instantiation. The complete system: universal hardware, automated tool chain, embedded system management, represents the "ideal" neural modelling environment: a general-purpose platform that can generate an arbitrary neural network and run it with hardware speed and scale. {\^{A}}© 2010 ACM.},
author = {Rast, A.D. and Jin, X. and Galluppi, F. and Plana, L.a. and Patterson, C. and Furber, S.},
doi = {ISBN:978-1-4503-0044-5},
file = {:Users/plaggm/Google Drive/ResearchPapers/Rast et al/Proceedings of the 7th ACM international conference on Computing frontiers/Rast et al.{\_}2010{\_}Scalable event-driven native parallel processing The spinnaker neuromimetic system(2).pdf:pdf},
isbn = {9781450300445},
journal = {Proceedings of the 7th ACM international conference on Computing frontiers},
keywords = {asynchronous,event-driven,universal neural processor},
number = {June 2015},
pages = {21--30},
title = {{Scalable event-driven native parallel processing: The spinnaker neuromimetic system}},
url = {http://dl.acm.org/citation.cfm?id=1787279},
year = {2010}
}
@article{Rast2010,
abstract = {Neural networks present a fundamentally different model of computation from the conventional sequential digital model. Modelling large networks on conventional hardware thus tends to be inefficient if not impossible. Neither dedicated neural chips, with model limitations, nor FPGA implementations, with scalability limitations, offer a satisfactory solution even though they have improved simulation performance dramatically. SpiNNaker introduces a different approach, the "neuromimetic" architecture, that maintains the neural optimisation of dedicated chips while offering FPGA-like universal configurability. Central to this parallel multiprocessor is an asynchronous event-driven model that uses interrupt-generating dedicated hardware on the chip to support real-time neural simulation. While this architecture is particularly suitable for spiking models, it can also implement "classical" neural models like the MLP efficiently. Nonetheless, event handling, particularly servicing incoming packets, requires careful and innovative design in order to avoid local processor congestion and possible deadlock. Using two exemplar models, a spiking network using Izhikevich neurons, and an MLP network, we illustrate how to implement efficient service routines to handle input events. These routines form the beginnings of a library of "drop-in" neural components. Ultimately, the goal is the creation of a library-based development system that allows the modeller to describe a model in a high-level neural description environment of his choice and use an automated tool chain to create the appropriate SpiNNaker instantiation. The complete system: universal hardware, automated tool chain, embedded system management, represents the "ideal" neural modelling environment: a general-purpose platform that can generate an arbitrary neural network and run it with hardware speed and scale. {\^{A}}© 2010 ACM.},
author = {Rast, A.D. and Jin, X. and Galluppi, F. and Plana, L.a. and Patterson, C. and Furber, S.},
doi = {ISBN:978-1-4503-0044-5},
file = {:Users/plaggm/Google Drive/ResearchPapers/Rast et al/Proceedings of the 7th ACM international conference on Computing frontiers/Rast et al.{\_}2010{\_}Scalable event-driven native parallel processing The spinnaker neuromimetic system.pdf:pdf},
isbn = {9781450300445},
journal = {Proceedings of the 7th ACM international conference on Computing frontiers},
number = {June 2015},
pages = {21--30},
title = {{Scalable event-driven native parallel processing: The spinnaker neuromimetic system}},
url = {http://dl.acm.org/citation.cfm?id=1787279},
year = {2010}
}
@article{Reutimann2003,
abstract = {We present a new technique, based on a proposed event-based strategy (Mattia {\&} Del Giudice, 2000), for efficiently simulating large networks of simple model neurons. The strategy was based on the fact that interactions among neurons occur by means of events that are well localized in time (the action potentials) and relatively rare. In the interval between two of these events, the state variables associated with a model neuron or a synapse evolved deterministically and in a predictable way. Here, we extend the event-driven simulation strategy to the case in which the dynamics of the state variables in the inter-event intervals are stochastic. This extension captures both the situation in which the simulated neurons are inherently noisy and the case in which they are embedded in a very large network and receive a huge number of random synaptic inputs. We show how to effectively include the impact of large background populations into neuronal dynamics by means of the numerical evaluation of the statistical properties of single-model neurons under random current injection. The new simulation strategy allows the study of networks of interacting neurons with an arbitrary number of external afferents and inherent stochastic dynamics.},
author = {Reutimann, Jan and Giugliano, Michele and Fusi, Stefano},
doi = {10.1162/08997660360581912},
file = {:Users/plaggm/Google Drive/ResearchPapers/Reutimann, Giugliano, Fusi/Neural computation/Reutimann, Giugliano, Fusi{\_}2003{\_}Event-driven simulation of spiking neurons with stochastic dynamics.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Action Potentials,Action Potentials: physiology,Algorithms,Computer Simulation,Evoked Potentials,Evoked Potentials: physiology,Models, Neurological,Motor Neurons,Motor Neurons: physiology,Neurons, Afferent,Neurons, Afferent: physiology,Stochastic Processes},
number = {4},
pages = {811--830},
pmid = {12689388},
title = {{Event-driven simulation of spiking neurons with stochastic dynamics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12689388},
volume = {15},
year = {2003}
}
@article{Rochel2003a,
author = {Rochel, Olivier and Martinez, Dominique},
file = {:Users/plaggm/Google Drive/ResearchPapers/Rochel, Martinez/ESANN'2003 Proceedings --European Symposium on Artifical Neural Networks/Rochel, Martinez{\_}2003{\_}An event-driven framework for the simulation of networks of spiking neurons(2).pdf:pdf},
isbn = {293030703X},
journal = {ESANN'2003 Proceedings --European Symposium on Artifical Neural Networks},
number = {JANUARY 2003},
pages = {295--300},
title = {{An event-driven framework for the simulation of networks of spiking neurons}},
year = {2003}
}
@article{Rochel2003,
author = {Rochel, Olivier and Martinez, Dominique},
file = {:Users/plaggm/Google Drive/ResearchPapers/Rochel, Martinez/ESANN'2003 Proceedings --European Symposium on Artifical Neural Networks/Rochel, Martinez{\_}2003{\_}An event-driven framework for the simulation of networks of spiking neurons.pdf:pdf},
isbn = {293030703X},
journal = {ESANN'2003 Proceedings --European Symposium on Artifical Neural Networks},
number = {JANUARY 2003},
pages = {295--300},
title = {{An event-driven framework for the simulation of networks of spiking neurons}},
year = {2003}
}
@article{Rudolph2006,
abstract = {Event-driven simulation strategies were proposed recently to simulate integrate-and-fire (IF) type neuronal models. These strategies can lead to computationally efficient algorithms for simulating large-scale networks of neurons; most important, such approaches are more precise than traditional clock-driven numerical integration approaches because the timing of spikes is treated exactly. The drawback of such event-driven methods is that in order to be efficient, the membrane equations must be solvable analytically, or at least provide simple analytic approximations for the state variables describing the system. This requirement prevents, in general, the use of conductance-based synaptic interactions within the framework of event-driven simulations and, thus, the investigation of network paradigms where synaptic conductances are important. We propose here a number of extensions of the classical leaky IF neuron model involving approximations of the membrane equation with conductance-based synaptic current, which lead to simple analytic expressions for the membrane state, and therefore can be used in the event-driven framework. These conductance-based IF (gIF) models are compared to commonly used models, such as the leaky IF model or biophysical models in which conductances are explicitly integrated. All models are compared with respect to various spiking response properties in the presence of synaptic activity, such as the spontaneous discharge statistics, the temporal precision in resolving synaptic inputs, and gain modulation under in vivo-like synaptic bombardment. Being based on the passive membrane equation with fixed-threshold spike generation, the proposed gIF models are situated in between leaky IF and biophysical models but are much closer to the latter with respect to their dynamic behavior and response characteristics, while still being nearly as computationally efficient as simple IF neuron models. gIF models should therefore provide a useful tool for efficient and precise simulation of large-scale neuronal networks with realistic, conductance-based synaptic interactions.},
author = {Rudolph, Michelle and Destexhe, Alain},
doi = {10.1186/1471-2202-10-S1-P23},
file = {:Users/plaggm/Google Drive/ResearchPapers/Rudolph, Destexhe/Neural computation/Rudolph, Destexhe{\_}2006{\_}Analytical integrate-and-fire neuron models with conductance-based dynamics for event-driven simulation strategie.pdf:pdf},
isbn = {0899-7667},
issn = {1471-2202},
journal = {Neural computation},
number = {9},
pages = {2146--2210},
pmid = {16846390},
title = {{Analytical integrate-and-fire neuron models with conductance-based dynamics for event-driven simulation strategies.}},
volume = {18},
year = {2006}
}
@article{Schliebs2013,
abstract = {This paper provides a comprehensive literature survey on the evolving Spiking Neural Network (eSNN) architecture since its introduction in 2006 as a further extension of the ECoS paradigm introduced by Kasabov in 1998. We summarize the functioning of the method, dis- cuss several of its extensions and present a number of applications in which the eSNN method was employed. We focus especially on some proposed extensions that allow the processing of spatio-temporal data and for feature and parameter optimisation of eSNN models to achieve better accuracy on classification/prediction problems and to facilitate new knowledge discovery. Finally, some open problems are discussed and future directions highlighted.},
author = {Schliebs, Stefan and Kasabov, Nikola},
doi = {10.1007/s12530-013-9074-9},
file = {:Users/plaggm/Google Drive/ResearchPapers/Schliebs, Kasabov/Evolving Systems/Schliebs, Kasabov{\_}2013{\_}Evolving spiking neural network - a survey.pdf:pdf},
issn = {18686478},
journal = {Evolving Systems},
keywords = {Evolving Connectionist Systems,Evolving Spiking Neural Network,spatio-temporal pattern recognition},
pages = {87--98},
title = {{Evolving spiking neural network - a survey}},
volume = {4},
year = {2013}
}
@article{Schmidh\uber2014,
abstract = {In recent years, deep neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidh{\{}$\backslash$"u{\}}ber, Juergen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:Users/plaggm/Google Drive/ResearchPapers/Schmidh{\{}u{\}}ber/arXiv preprint arXiv {\ldots}/Schmidh{\{}u{\}}ber{\_}2014{\_}Deep Learning in Neural Networks An Overview.pdf:pdf},
issn = {18792782},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {66},
title = {{Deep Learning in Neural Networks: An Overview}},
url = {http://arxiv.org/abs/1404.7828},
volume = {abs/1404.7},
year = {2014}
}
@article{Smotherman2009,
author = {Smotherman, Mark},
file = {:Users/plaggm/Google Drive/ResearchPapers/Smotherman/Unknown/Smotherman{\_}2009{\_}Acceleration of Spiking Neural Networks on Multicore.pdf:pdf},
number = {August},
title = {{Acceleration of Spiking Neural Networks on Multicore}},
year = {2009}
}
@article{2015arXiv150503015S,
archivePrefix = {arXiv},
arxivId = {cs.DC/1505.03015},
author = {{Stanislao Paolucci}, P and Ammendola, R and Biagioni, A and Frezza, O and {Lo Cicero}, F and Lonardo, A and Martinelli, M and Pastorelli, E and Simula, F and Vicini, P},
eprint = {1505.03015},
file = {:Users/plaggm/Google Drive/ResearchPapers/Stanislao Paolucci et al/ArXiv e-prints/Stanislao Paolucci et al.{\_}2015{\_}Power, Energy and Speed of Embedded and Server Multi-Cores applied to Distributed Simulation of Spiking N.pdf:pdf},
journal = {ArXiv e-prints},
keywords = {C.1.4,C.2.4,Computer Science - Distributed,Parallel,Quantitative Biology - Neurons and Cognition,and Cluster Computing},
month = {may},
primaryClass = {cs.DC},
title = {{Power, Energy and Speed of Embedded and Server Multi-Cores applied to Distributed Simulation of Spiking Neural Networks: ARM in NVIDIA Tegra vs Intel Xeon quad-cores}},
year = {2015}
}
@article{Stewart2009,
author = {Stewart, Robert D. and Bair, Wyeth},
doi = {10.1007/s10827-008-0131-5},
file = {:Users/plaggm/Google Drive/ResearchPapers/Stewart, Bair/Journal of Computational Neuroscience/Stewart, Bair{\_}2009{\_}Spiking neural network simulation numerical integration with the Parker-Sochacki method.pdf:pdf},
isbn = {1082700801315},
issn = {0929-5313},
journal = {Journal of Computational Neuroscience},
keywords = {2007,and,hodgkin-huxley,izhikevich,numerical integration,or firing events,parker-sochacki,punctuated by discrete synaptic,spiking neural network,the state variables,with continuous evolution of},
number = {1},
pages = {115--133},
title = {{Spiking neural network simulation: numerical integration with the Parker-Sochacki method}},
url = {http://link.springer.com/10.1007/s10827-008-0131-5},
volume = {27},
year = {2009}
}
@article{Thulasidasan2006,
author = {Thulasidasan, Sunil and Kroc, Lukas and Eidenbenz, Stephan},
file = {:Users/plaggm/Google Drive/ResearchPapers/Thulasidasan, Kroc, Eidenbenz/Unknown/Thulasidasan, Kroc, Eidenbenz{\_}2006{\_}Developing Parallel , Discrete Event Simulations in Python First Results and User Experiences with.pdf:pdf},
title = {{Developing Parallel , Discrete Event Simulations in Python : First Results and User Experiences with the SimX Library ∗}},
year = {2006}
}
@article{Wilke2014,
author = {Wilke, Jeremiah J and Kenny, Joseph P},
file = {:Users/plaggm/Google Drive/ResearchPapers/Wilke, Kenny/Unknown/Wilke, Kenny{\_}2014{\_}Using Discrete Event Simulation for Programming Model Exploration at Extreme-Scale Macroscale Components for the Stru.pdf:pdf},
number = {October},
title = {{Using Discrete Event Simulation for Programming Model Exploration at Extreme-Scale : Macroscale Components for the Structural Simulation Toolkit ( SST )}},
year = {2014}
}
@article{Xiao2015,
author = {Xiao, Wei-cheng and Zhao, Jisheng},
file = {:Users/plaggm/Google Drive/ResearchPapers/Xiao, Zhao/20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming/Xiao, Zhao{\_}2015{\_}Parallelizing a Discrete Event Simulation Application Using the Habanero-Java Multicore Library Categories and Subject D.pdf:pdf},
isbn = {9781450334044},
journal = {20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
keywords = {department of computer,rice university,science},
title = {{Parallelizing a Discrete Event Simulation Application Using the Habanero-Java Multicore Library Categories and Subject Descriptors}},
year = {2015}
}
